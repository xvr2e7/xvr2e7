{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_pretraining_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMasFMzf4Z8m+23M8oWR1EH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziyan-xvx/ziyan-xvx/blob/main/build_pretraining_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g_Cq46OoJbP"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2020 The Google Research Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Writes out text data as tfrecords that ELECTRA can be pre-trained on.\"\"\"\n",
        "\n",
        "import argparse\n",
        "import multiprocessing\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from model import tokenization\n",
        "from util import utils\n",
        "\n",
        "\n",
        "def create_int_feature(values):\n",
        "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "  return feature\n",
        "\n",
        "\n",
        "class ExampleBuilder(object):\n",
        "  \"\"\"Given a stream of input text, creates pretraining examples.\"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer, max_length):\n",
        "    self._tokenizer = tokenizer\n",
        "    self._current_sentences = []\n",
        "    self._current_length = 0\n",
        "    self._max_length = max_length\n",
        "    self._target_length = max_length\n",
        "\n",
        "  def add_line(self, line):\n",
        "    \"\"\"Adds a line of text to the current example being built.\"\"\"\n",
        "    line = line.strip().replace(\"\\n\", \" \")\n",
        "    if (not line) and self._current_length != 0:  # empty lines separate docs\n",
        "      return self._create_example()\n",
        "    bert_tokens = self._tokenizer.tokenize(line)\n",
        "    bert_tokids = self._tokenizer.convert_tokens_to_ids(bert_tokens)\n",
        "    self._current_sentences.append(bert_tokids)\n",
        "    self._current_length += len(bert_tokids)\n",
        "    if self._current_length >= self._target_length:\n",
        "      return self._create_example()\n",
        "    return None\n",
        "\n",
        "  def _create_example(self):\n",
        "    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n",
        "    # small chance to only have one segment as in classification tasks\n",
        "    if random.random() < 0.1:\n",
        "      first_segment_target_length = 100000\n",
        "    else:\n",
        "      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n",
        "      first_segment_target_length = (self._target_length - 3) // 2\n",
        "\n",
        "    first_segment = []\n",
        "    second_segment = []\n",
        "    for sentence in self._current_sentences:\n",
        "      # the sentence goes to the first segment if (1) the first segment is\n",
        "      # empty, (2) the sentence doesn't put the first segment over length or\n",
        "      # (3) 50% of the time when it does put the first segment over length\n",
        "      if (len(first_segment) == 0 or\n",
        "          len(first_segment) + len(sentence) < first_segment_target_length or\n",
        "          (len(second_segment) == 0 and\n",
        "           len(first_segment) < first_segment_target_length and\n",
        "           random.random() < 0.5)):\n",
        "        first_segment += sentence\n",
        "      else:\n",
        "        second_segment += sentence\n",
        "\n",
        "    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n",
        "    first_segment = first_segment[:self._max_length - 2]\n",
        "    second_segment = second_segment[:max(0, self._max_length -\n",
        "                                         len(first_segment) - 3)]\n",
        "\n",
        "    # prepare to start building the next example\n",
        "    self._current_sentences = []\n",
        "    self._current_length = 0\n",
        "    # small chance for random-length instead of max_length-length example\n",
        "    if random.random() < 0.05:\n",
        "      self._target_length = random.randint(5, self._max_length)\n",
        "    else:\n",
        "      self._target_length = self._max_length\n",
        "\n",
        "    return self._make_tf_example(first_segment, second_segment)\n",
        "\n",
        "  def _make_tf_example(self, first_segment, second_segment):\n",
        "    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n",
        "    vocab = self._tokenizer.vocab\n",
        "    input_ids = [vocab[\"[CLS]\"]] + first_segment + [vocab[\"[SEP]\"]]\n",
        "    segment_ids = [0] * len(input_ids)\n",
        "    if second_segment:\n",
        "      input_ids += second_segment + [vocab[\"[SEP]\"]]\n",
        "      segment_ids += [1] * (len(second_segment) + 1)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    input_ids += [0] * (self._max_length - len(input_ids))\n",
        "    input_mask += [0] * (self._max_length - len(input_mask))\n",
        "    segment_ids += [0] * (self._max_length - len(segment_ids))\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        \"input_ids\": create_int_feature(input_ids),\n",
        "        \"input_mask\": create_int_feature(input_mask),\n",
        "        \"segment_ids\": create_int_feature(segment_ids)\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "class ExampleWriter(object):\n",
        "  \"\"\"Writes pre-training examples to disk.\"\"\"\n",
        "\n",
        "  def __init__(self, job_id, vocab_file, output_dir, max_seq_length,\n",
        "               num_jobs, blanks_separate_docs, do_lower_case,\n",
        "               num_out_files=1000, strip_accents=True):\n",
        "    self._blanks_separate_docs = blanks_separate_docs\n",
        "    tokenizer = tokenization.FullTokenizer(\n",
        "        vocab_file=vocab_file,\n",
        "        do_lower_case=do_lower_case,\n",
        "        strip_accents=strip_accents)\n",
        "    self._example_builder = ExampleBuilder(tokenizer, max_seq_length)\n",
        "    self._writers = []\n",
        "    for i in range(num_out_files):\n",
        "      if i % num_jobs == job_id:\n",
        "        output_fname = os.path.join(\n",
        "            output_dir, \"pretrain_data.tfrecord-{:}-of-{:}\".format(\n",
        "                i, num_out_files))\n",
        "        self._writers.append(tf.io.TFRecordWriter(output_fname))\n",
        "    self.n_written = 0\n",
        "\n",
        "  def write_examples(self, input_file):\n",
        "    \"\"\"Writes out examples from the provided input file.\"\"\"\n",
        "    with tf.io.gfile.GFile(input_file) as f:\n",
        "      for line in f:\n",
        "        line = line.strip()\n",
        "        if line or self._blanks_separate_docs:\n",
        "          example = self._example_builder.add_line(line)\n",
        "          if example:\n",
        "            self._writers[self.n_written % len(self._writers)].write(\n",
        "                example.SerializeToString())\n",
        "            self.n_written += 1\n",
        "      example = self._example_builder.add_line(\"\")\n",
        "      if example:\n",
        "        self._writers[self.n_written % len(self._writers)].write(\n",
        "            example.SerializeToString())\n",
        "        self.n_written += 1\n",
        "\n",
        "  def finish(self):\n",
        "    for writer in self._writers:\n",
        "      writer.close()\n",
        "\n",
        "\n",
        "def write_examples(job_id, args):\n",
        "  \"\"\"A single process creating and writing out pre-processed examples.\"\"\"\n",
        "\n",
        "  def log(*args):\n",
        "    msg = \" \".join(map(str, args))\n",
        "    print(\"Job {}:\".format(job_id), msg)\n",
        "\n",
        "  log(\"Creating example writer\")\n",
        "  example_writer = ExampleWriter(\n",
        "      job_id=job_id,\n",
        "      vocab_file=args.vocab_file,\n",
        "      output_dir=args.output_dir,\n",
        "      max_seq_length=args.max_seq_length,\n",
        "      num_jobs=args.num_processes,\n",
        "      blanks_separate_docs=args.blanks_separate_docs,\n",
        "      do_lower_case=args.do_lower_case,\n",
        "      strip_accents=args.strip_accents,\n",
        "  )\n",
        "  log(\"Writing tf examples\")\n",
        "  fnames = sorted(tf.io.gfile.listdir(args.corpus_dir))\n",
        "  fnames = [f for (i, f) in enumerate(fnames)\n",
        "            if i % args.num_processes == job_id]\n",
        "  random.shuffle(fnames)\n",
        "  start_time = time.time()\n",
        "  for file_no, fname in enumerate(fnames):\n",
        "    if file_no > 0:\n",
        "      elapsed = time.time() - start_time\n",
        "      log(\"processed {:}/{:} files ({:.1f}%), ELAPSED: {:}s, ETA: {:}s, \"\n",
        "          \"{:} examples written\".format(\n",
        "              file_no, len(fnames), 100.0 * file_no / len(fnames), int(elapsed),\n",
        "              int((len(fnames) - file_no) / (file_no / elapsed)),\n",
        "              example_writer.n_written))\n",
        "    example_writer.write_examples(os.path.join(args.corpus_dir, fname))\n",
        "  example_writer.finish()\n",
        "  log(\"Done!\")\n",
        "\n",
        "\n",
        "def main():\n",
        "  parser = argparse.ArgumentParser(description=__doc__)\n",
        "  parser.add_argument(\"--corpus-dir\", required=True,\n",
        "                      help=\"Location of pre-training text files.\")\n",
        "  parser.add_argument(\"--vocab-file\", required=True,\n",
        "                      help=\"Location of vocabulary file.\")\n",
        "  parser.add_argument(\"--output-dir\", required=True,\n",
        "                      help=\"Where to write out the tfrecords.\")\n",
        "  parser.add_argument(\"--max-seq-length\", default=128, type=int,\n",
        "                      help=\"Number of tokens per example.\")\n",
        "  parser.add_argument(\"--num-processes\", default=1, type=int,\n",
        "                      help=\"Parallelize across multiple processes.\")\n",
        "  parser.add_argument(\"--blanks-separate-docs\", default=True, type=bool,\n",
        "                      help=\"Whether blank lines indicate document boundaries.\")\n",
        "\n",
        "  # toggle lower-case\n",
        "  parser.add_argument(\"--do-lower-case\", dest='do_lower_case',\n",
        "                      action='store_true', help=\"Lower case input text.\")\n",
        "  parser.add_argument(\"--no-lower-case\", dest='do_lower_case',\n",
        "                      action='store_false', help=\"Don't lower case input text.\")\n",
        "\n",
        "  # toggle strip-accents\n",
        "  parser.add_argument(\"--do-strip-accents\", dest='strip_accents',\n",
        "                      action='store_true', help=\"Strip accents (default).\")\n",
        "  parser.add_argument(\"--no-strip-accents\", dest='strip_accents',\n",
        "                      action='store_false', help=\"Don't strip accents.\")\n",
        "\n",
        "  # set defaults for toggles\n",
        "  parser.set_defaults(do_lower_case=True)\n",
        "  parser.set_defaults(strip_accents=True)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  utils.rmkdir(args.output_dir)\n",
        "  if args.num_processes == 1:\n",
        "    write_examples(0, args)\n",
        "  else:\n",
        "    jobs = []\n",
        "    for i in range(args.num_processes):\n",
        "      job = multiprocessing.Process(target=write_examples, args=(i, args))\n",
        "      jobs.append(job)\n",
        "      job.start()\n",
        "    for job in jobs:\n",
        "      job.join()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}